{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    " \n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    " \n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    return supervised\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True, return_sequences=True))\n",
    "    model.add(LSTM(32, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(RepeatVector(3))\n",
    "    model.add(LSTM(128, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True, return_sequences=True))\n",
    "    model.add(LSTM(32, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_encoder(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast, state_h, state_c = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return state_c\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts_encoder(model, n_batch, train, test, n_lag, n_seq):\n",
    "    states = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        state_c = forecast_encoder(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        states.append(state_c)\n",
    "    return states\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    " \n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    " \n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        smape = mean_absolute_percentage_error(actual, predicted)\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "        print('t+%d SMAPE: %f' % ((i+1), smape))\n",
    "\n",
    "# SMAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / ((y_true + y_pred) / 2))) * 100\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    pyplot.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + forecasts[i]\n",
    "        pyplot.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "n_lag = 12\n",
    "n_seq = 1\n",
    "n_test = 22\n",
    "n_epochs = 1500\n",
    "n_batch = 1\n",
    "n_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "series = read_csv('shampoo_sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-12)</th>\n",
       "      <th>var1(t-11)</th>\n",
       "      <th>var1(t-10)</th>\n",
       "      <th>var1(t-9)</th>\n",
       "      <th>var1(t-8)</th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var1(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.639992</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>-0.405945</td>\n",
       "      <td>0.112866</td>\n",
       "      <td>-0.189773</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>-0.171066</td>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.013926</td>\n",
       "      <td>-0.405945</td>\n",
       "      <td>0.112866</td>\n",
       "      <td>-0.189773</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>-0.171066</td>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.405945</td>\n",
       "      <td>0.112866</td>\n",
       "      <td>-0.189773</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>-0.171066</td>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.112866</td>\n",
       "      <td>-0.189773</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>-0.171066</td>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.189773</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>-0.171066</td>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.122428</td>\n",
       "      <td>-0.171066</td>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.171066</td>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.272501</td>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.431303</td>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.747246</td>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.766784</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.326959</td>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.122012</td>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.481189</td>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.256703</td>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "      <td>0.009354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.394305</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.433382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.181875</td>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.433382</td>\n",
       "      <td>-0.838703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.197672</td>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.433382</td>\n",
       "      <td>-0.838703</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.406776</td>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.433382</td>\n",
       "      <td>-0.838703</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.793806</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.433382</td>\n",
       "      <td>-0.838703</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.299938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.182706</td>\n",
       "      <td>-0.151528</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>-0.658283</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>-0.298691</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.433382</td>\n",
       "      <td>-0.838703</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.299938</td>\n",
       "      <td>0.131989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-12)  var1(t-11)  var1(t-10)  var1(t-9)  var1(t-8)  var1(t-7)  \\\n",
       "12   -0.639992    0.013926   -0.405945   0.112866  -0.189773   0.122428   \n",
       "13    0.013926   -0.405945    0.112866  -0.189773   0.122428  -0.171066   \n",
       "14   -0.405945    0.112866   -0.189773   0.122428  -0.171066  -0.272501   \n",
       "15    0.112866   -0.189773    0.122428  -0.171066  -0.272501  -0.431303   \n",
       "16   -0.189773    0.122428   -0.171066  -0.272501  -0.431303   0.747246   \n",
       "17    0.122428   -0.171066   -0.272501  -0.431303   0.747246  -0.766784   \n",
       "18   -0.171066   -0.272501   -0.431303   0.747246  -0.766784  -0.105799   \n",
       "19   -0.272501   -0.431303    0.747246  -0.766784  -0.105799  -0.326959   \n",
       "20   -0.431303    0.747246   -0.766784  -0.105799  -0.326959   0.111203   \n",
       "21    0.747246   -0.766784   -0.105799  -0.326959   0.111203   0.122012   \n",
       "22   -0.766784   -0.105799   -0.326959   0.111203   0.122012  -0.481189   \n",
       "23   -0.105799   -0.326959    0.111203   0.122012  -0.481189   0.256703   \n",
       "24   -0.326959    0.111203    0.122012  -0.481189   0.256703  -0.394305   \n",
       "25    0.111203    0.122012   -0.481189   0.256703  -0.394305   0.181875   \n",
       "26    0.122012   -0.481189    0.256703  -0.394305   0.181875  -0.197672   \n",
       "27   -0.481189    0.256703   -0.394305   0.181875  -0.197672   0.406776   \n",
       "28    0.256703   -0.394305    0.181875  -0.197672   0.406776  -0.793806   \n",
       "29   -0.394305    0.181875   -0.197672   0.406776  -0.793806   0.182706   \n",
       "30    0.181875   -0.197672    0.406776  -0.793806   0.182706  -0.151528   \n",
       "31   -0.197672    0.406776   -0.793806   0.182706  -0.151528   0.277905   \n",
       "32    0.406776   -0.793806    0.182706  -0.151528   0.277905  -0.658283   \n",
       "33   -0.793806    0.182706   -0.151528   0.277905  -0.658283   0.372272   \n",
       "34    0.182706   -0.151528    0.277905  -0.658283   0.372272  -0.298691   \n",
       "\n",
       "    var1(t-6)  var1(t-5)  var1(t-4)  var1(t-3)  var1(t-2)  var1(t-1)   var1(t)  \n",
       "12  -0.171066  -0.272501  -0.431303   0.747246  -0.766784  -0.105799 -0.326959  \n",
       "13  -0.272501  -0.431303   0.747246  -0.766784  -0.105799  -0.326959  0.111203  \n",
       "14  -0.431303   0.747246  -0.766784  -0.105799  -0.326959   0.111203  0.122012  \n",
       "15   0.747246  -0.766784  -0.105799  -0.326959   0.111203   0.122012 -0.481189  \n",
       "16  -0.766784  -0.105799  -0.326959   0.111203   0.122012  -0.481189  0.256703  \n",
       "17  -0.105799  -0.326959   0.111203   0.122012  -0.481189   0.256703 -0.394305  \n",
       "18  -0.326959   0.111203   0.122012  -0.481189   0.256703  -0.394305  0.181875  \n",
       "19   0.111203   0.122012  -0.481189   0.256703  -0.394305   0.181875 -0.197672  \n",
       "20   0.122012  -0.481189   0.256703  -0.394305   0.181875  -0.197672  0.406776  \n",
       "21  -0.481189   0.256703  -0.394305   0.181875  -0.197672   0.406776 -0.793806  \n",
       "22   0.256703  -0.394305   0.181875  -0.197672   0.406776  -0.793806  0.182706  \n",
       "23  -0.394305   0.181875  -0.197672   0.406776  -0.793806   0.182706 -0.151528  \n",
       "24   0.181875  -0.197672   0.406776  -0.793806   0.182706  -0.151528  0.277905  \n",
       "25  -0.197672   0.406776  -0.793806   0.182706  -0.151528   0.277905 -0.658283  \n",
       "26   0.406776  -0.793806   0.182706  -0.151528   0.277905  -0.658283  0.372272  \n",
       "27  -0.793806   0.182706  -0.151528   0.277905  -0.658283   0.372272 -0.298691  \n",
       "28   0.182706  -0.151528   0.277905  -0.658283   0.372272  -0.298691  0.009354  \n",
       "29  -0.151528   0.277905  -0.658283   0.372272  -0.298691   0.009354  0.433382  \n",
       "30   0.277905  -0.658283   0.372272  -0.298691   0.009354   0.433382 -0.838703  \n",
       "31  -0.658283   0.372272  -0.298691   0.009354   0.433382  -0.838703  1.000000  \n",
       "32   0.372272  -0.298691   0.009354   0.433382  -0.838703   1.000000 -1.000000  \n",
       "33  -0.298691   0.009354   0.433382  -0.838703   1.000000  -1.000000  0.299938  \n",
       "34   0.009354   0.433382  -0.838703   1.000000  -1.000000   0.299938  0.131989  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prepare_data(series,n_test,n_lag,n_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27250052 -0.43130326  0.74724589 -0.76678445 -0.10579921 -0.32695905\n",
      "   0.11120349  0.12201206 -0.48118894  0.25670339 -0.39430472  0.18187487\n",
      "  -0.197672  ]\n",
      " [-0.43130326  0.74724589 -0.76678445 -0.10579921 -0.32695905  0.11120349\n",
      "   0.12201206 -0.48118894  0.25670339 -0.39430472  0.18187487 -0.197672\n",
      "   0.40677614]\n",
      " [ 0.74724589 -0.76678445 -0.10579921 -0.32695905  0.11120349  0.12201206\n",
      "  -0.48118894  0.25670339 -0.39430472  0.18187487 -0.197672    0.40677614\n",
      "  -0.79380586]\n",
      " [-0.76678445 -0.10579921 -0.32695905  0.11120349  0.12201206 -0.48118894\n",
      "   0.25670339 -0.39430472  0.18187487 -0.197672    0.40677614 -0.79380586\n",
      "   0.1827063 ]\n",
      " [-0.10579921 -0.32695905  0.11120349  0.12201206 -0.48118894  0.25670339\n",
      "  -0.39430472  0.18187487 -0.197672    0.40677614 -0.79380586  0.1827063\n",
      "  -0.15152775]\n",
      " [-0.32695905  0.11120349  0.12201206 -0.48118894  0.25670339 -0.39430472\n",
      "   0.18187487 -0.197672    0.40677614 -0.79380586  0.1827063  -0.15152775\n",
      "   0.2779048 ]\n",
      " [ 0.11120349  0.12201206 -0.48118894  0.25670339 -0.39430472  0.18187487\n",
      "  -0.197672    0.40677614 -0.79380586  0.1827063  -0.15152775  0.2779048\n",
      "  -0.6582831 ]\n",
      " [ 0.12201206 -0.48118894  0.25670339 -0.39430472  0.18187487 -0.197672\n",
      "   0.40677614 -0.79380586  0.1827063  -0.15152775  0.2779048  -0.6582831\n",
      "   0.37227188]]\n",
      "[-0.197672    0.40677614 -0.79380586  0.1827063  -0.15152775  0.2779048\n",
      " -0.6582831   0.37227188]\n"
     ]
    }
   ],
   "source": [
    "#prepare train-test sets\n",
    "df = prepare_data(series,n_test,n_lag,n_seq)\n",
    "# display(df)\n",
    "values = df.values\n",
    "encoder_train = values[0:7]\n",
    "encoder_test_bnn_train = values[7:15]\n",
    "bnn_test = values[15:23]\n",
    "state_y_train = encoder_test_bnn_train[:,-1]\n",
    "state_y_test = bnn_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = fit_lstm(encoder_train, n_lag, n_seq, n_batch, n_epochs, n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(model, train, n_lag, n_batch):\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    input1 = Input(batch_shape=(n_batch, X.shape[1], X.shape[2]))\n",
    "    x = LSTM(128, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True, return_sequences=True)(input1)\n",
    "    x, state_h, state_c = LSTM(32, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True, return_state=True)(x)\n",
    "    encoder = Model(inputs=input1, outputs=[x, state_h, state_c])\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = extract_embedding(lstm, encoder_train, n_lag, n_batch)\n",
    "states = make_forecasts_encoder(encoder, n_batch, encoder_train, encoder_test_bnn_train, n_lag, n_seq)\n",
    "for i in range(len(states)):\n",
    "    states[i] = np.append(states[i], state_y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an BNN network to training data\n",
    "def fit_bnn(train, n_input, n_batch, nb_epoch, n_neurons):\n",
    "    X = [row[0:-1] for row in train]\n",
    "    y = [row[-1] for row in train]\n",
    "    X = np.array(X)\n",
    "    # Hardcode\n",
    "    input1 = Input(shape=(n_input,))\n",
    "    x = Dense(128, activation=\"tanh\")(input1)\n",
    "    x = Dropout(0.5)(x, training=True)\n",
    "    x = Dense(64, activation=\"tanh\")(x)\n",
    "    x = Dropout(0.5)(x, training=True)\n",
    "    x = Dense(16, activation=\"tanh\")(x)\n",
    "    x = Dropout(0.5)(x, training=True)\n",
    "    x = Dense(1)(x)\n",
    "    bnn = Model(inputs=[input1], outputs=[x])\n",
    "    bnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        bnn.fit(X, y, epochs=1, verbose=0, shuffle=False)\n",
    "    return bnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 32)\n"
     ]
    }
   ],
   "source": [
    "states = np.array(states)\n",
    "bnn = fit_bnn(states, 32, n_batch, n_epochs, n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,537\n",
      "Trainable params: 13,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an bnn,\n",
    "def forecast_bnn(model, X):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts_bnn(model, train):\n",
    "    forecasts = list()\n",
    "    X = [row[0:-1] for row in train]\n",
    "    y = [row[-1] for row in train]\n",
    "    X = np.array(X)\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test)\n",
    "# evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "# plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ddc18b1abcfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "print train\n",
    "# fit model\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
